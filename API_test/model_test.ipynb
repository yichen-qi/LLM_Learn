{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b30c9c56",
   "metadata": {},
   "source": [
    "##\n",
    "##\n",
    "导入模型\n",
    "pipline 管道函数 使用加载的模型和分词器创建生成文本的pipeline\n",
    "output = generator(\n",
    "    \"输入文本\",\n",
    "    max_length, 输入加输出总长度\n",
    "    num_return_sequences, 生成几条不同的结果\n",
    "    no_repeat_ngram_size, 防止重复出现的 n-gram，设为 1 表示 禁止重复单词，设为 2 表示禁止生成连续两个相同的词组（比如“我喜欢我喜欢…”）\n",
    "    repetition_penalty, 重复惩罚系数\n",
    "    temperature, 控制生成结果的多样性，低温度（如 0.7）会使模型生成更保守、更确定性的内容\n",
    "    top_k = 50, 在每一步生成中只从概率最高的前 k 个 token 中采样，设为 50 就是在每一步中从前 50 个候选词中随机选择\n",
    "    top_p = 0.9, 只从累积概率加起来前 90% 的 token 中随机抽样\n",
    "    clean_up_tokenization_spaces=False， 控制是否清理分词产生的空格或特殊字符，设置为 False 表示保留原样（调试时更真实），为 True 则清理干净（适合展示\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e34ab12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '你好， 我是一名学生， 我想问一下， 你们有什么收藏的电影吗？ 我 的 问 题 是 ， 想 看 看 大 家 收 集 的 经 典 电 视 剧 ， 以 及 其 中 的 片 段 。 我 在 电 脑 上 看 的 ， 没 有 找 到 ， 不 知 道 有 没 人 找 。 谢 谢 大 神 们 ， 谢 邀 我 看 过 的 所 有 电 子 书 都 是 这'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_dir = \"./model/uer/gpt2-chinese-cluecorpussmall/models--uer--gpt2-chinese-cluecorpussmall/snapshots/c2c0249d8a2731f269414cc3b22dff021f8e07a3\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "generater = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "output = generater(\n",
    "    \"你好， 我是一名学生， 我想问一下， 你们有什么收藏的电影吗？\",\n",
    "    max_length=100,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,\n",
    "    no_repeat_ngram_size = 2,\n",
    "    top_p=0.9,\n",
    "    top_k=20,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "\n",
    "output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ec302d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfb96e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./model/google-bert/bert-base-chinese/models--google-bert--bert-base-chinese/snapshots/c30a6ed22ab4564dc1e3b2ecbf6e766b0611a33f and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.5310388207435608}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, pipelines\n",
    "\n",
    "model_dir = \"./model/google-bert/bert-base-chinese/models--google-bert--bert-base-chinese/snapshots/c30a6ed22ab4564dc1e3b2ecbf6e766b0611a33f\"\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "classifier = pipeline('text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "result = classifier(\"你好, 我是一款语言模型\")\n",
    "\n",
    "result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
