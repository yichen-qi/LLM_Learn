{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMyt0NLG3YhMOnyGFQPwhiH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yichen-qi/LLM_Learn/blob/main/model_finetunning_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM模型微调，训练，测试，调用"
      ],
      "metadata": {
        "id": "EM3JpkayQxWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 依赖库"
      ],
      "metadata": {
        "id": "17QLmy7zRm89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n"
      ],
      "metadata": {
        "id": "h5y6lH1ER2iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 导入依赖所有库"
      ],
      "metadata": {
        "id": "4QEl-U2RT3RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import BertModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer\n",
        "from torch.optim import AdamW\n"
      ],
      "metadata": {
        "id": "8YIREDJjQyWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 制作Dataset"
      ],
      "metadata": {
        "id": "zMsZ5j_qUB_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, split):\n",
        "        self.dataset = load_dataset(\"lansinuote/ChnSentiCorp\")\n",
        "\n",
        "        if split == 'train':\n",
        "            self.data = self.dataset['train']\n",
        "        elif split == 'validation':\n",
        "            self.data = self.dataset['validation']\n",
        "        elif split == 'test':\n",
        "            self.data = self.dataset['test']\n",
        "        else:\n",
        "            raise ValueError('Invalid split')\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.data[item]['text'], self.data[item]['label']\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_dataset = CustomDataset('train')\n",
        "    print(len(train_dataset))\n",
        "    print(train_dataset[0])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Cp2aGPSzUJb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 下游任务模型设计"
      ],
      "metadata": {
        "id": "67xUpaGwU0EK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 加载预训练模型"
      ],
      "metadata": {
        "id": "VXgd85QPU66H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "bert_model = BertModel.from_pretrained(\"google-bert/bert-base-chinese\")\n",
        "bert_model.to(device)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gkEQmw_gU5SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 定义下游任务模型(将主干网络提取的特征进行分类)\n"
      ],
      "metadata": {
        "id": "DkyTS6gzVeSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.fc = torch.nn.Linear(768, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        #上游不参与训练\n",
        "        with torch.no_grad():\n",
        "            out = bert_model(input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids)\n",
        "\n",
        "        #下游参与训练\n",
        "\n",
        "        out = self.fc(out.last_hidden_state[:,0])\n",
        "        out = out.softmax(dim=1)\n",
        "        return out"
      ],
      "metadata": {
        "id": "UAjVaOpMViHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 自定义模型微调"
      ],
      "metadata": {
        "id": "q-QjXK3ZVxkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token = BertTokenizer.from_pretrained(\"google-bert/bert-base-chinese\")\n",
        "epochs = 100\n",
        "\n",
        "#数据编码处理\n",
        "def collate_fn(data):\n",
        "    sentences = [item[0] for item in data]\n",
        "    labels = [item[1] for item in data]\n",
        "\n",
        "    data = token.batch_encode_plus(\n",
        "        sentences,\n",
        "        padding=\"max_length\",\n",
        "        max_length=350,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "        return_length=True\n",
        "    )\n",
        "    input_ids = data[\"input_ids\"]\n",
        "    attention_mask = data[\"attention_mask\"]\n",
        "    token_type_ids = data[\"token_type_ids\"]\n",
        "    labels = torch.LongTensor(labels)\n",
        "    return input_ids, attention_mask, token_type_ids, labels\n",
        "\n",
        "train_dataset = CustomDataset('train')\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    batch_size = 32,\n",
        "    shuffle = True,\n",
        "    drop_last = True,\n",
        "    collate_fn = collate_fn\n",
        ")\n",
        "\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(device)\n",
        "    my_model = Model().to(device)\n",
        "\n",
        "    optimizer = AdamW(my_model.parameters(), lr=5e-4)\n",
        "\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    my_model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for step, (input_ids, attention_mask, token_type_ids, labels) in enumerate(train_loader):\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            token_type_ids = token_type_ids.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            out = my_model(input_ids, attention_mask, token_type_ids)\n",
        "\n",
        "            loss = loss_func(out, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if step % 10 == 0:\n",
        "                out = out.argmax(dim=1)\n",
        "                acc = (out == labels).sum().item() / len(labels)\n",
        "                print(f\"Epoch: {epoch}, Step: {step}, Loss: {loss.item()}, Acc: {acc}\")\n",
        "\n",
        "        torch.save(my_model.state_dict(), f\"params/{epoch}bert.pt\")\n",
        "        print(f\"Epoch: {epoch}, Save model params\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RsUf1h4rVx7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 模型性能测试\n"
      ],
      "metadata": {
        "id": "xhb5K5nwibrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = CustomDataset('test')\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    batch_size = 32,\n",
        "    shuffle = True,\n",
        "    drop_last = True,\n",
        "    collate_fn = collate_fn\n",
        ")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    acc = 0\n",
        "    total = 0\n",
        "\n",
        "    test_model = Model().to(device)\n",
        "    test_model.load_state_dict(torch.load('params/11bert.pt'))\n",
        "    test_model.eval()\n",
        "\n",
        "    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(test_loader):\n",
        "\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        token_type_ids = token_type_ids.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        out = test_model(input_ids, attention_mask, token_type_ids)\n",
        "\n",
        "        out = out.argmax(dim=1)\n",
        "        acc += (out == labels).sum().item()\n",
        "        total += len(labels)\n",
        "\n",
        "    print(\"Test Accuracy: {:.4f}\".format(acc/total))"
      ],
      "metadata": {
        "id": "44qrBUDDifK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 模型调用"
      ],
      "metadata": {
        "id": "LeoodZm8jEIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_model = Model().to(device)\n",
        "\n",
        "names = [\"负向评价\", \"正向评价\"]\n",
        "\n",
        "def collate_fn_sentiment(data):\n",
        "    sentences = data\n",
        "\n",
        "\n",
        "    data = token.batch_encode_plus(\n",
        "        sentences,\n",
        "        padding=\"max_length\",\n",
        "        max_length=350,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "        return_length=True\n",
        "    )\n",
        "\n",
        "    input_ids = data[\"input_ids\"]\n",
        "    attention_mask = data[\"attention_mask\"]\n",
        "    token_type_ids = data[\"token_type_ids\"]\n",
        "    return input_ids, attention_mask, token_type_ids\n",
        "\n",
        "\n",
        "def prediction():\n",
        "    sentiment_model.load_state_dict(torch.load('params/11bert.pt'))\n",
        "    sentiment_model.eval()\n",
        "\n",
        "    while True:\n",
        "        sentence = input(\"请输入句子(输入q退出)：\")\n",
        "        if sentence == \"q\":\n",
        "            print(\"退出\")\n",
        "            break\n",
        "\n",
        "        input_ids, attention_mask, token_type_ids = collate_fn_sentiment([sentence])\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        token_type_ids = token_type_ids.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = sentiment_model(input_ids, attention_mask, token_type_ids)\n",
        "            output = output.argmax(dim=1)\n",
        "            print(\"模型判定\", names[output], \"\\n\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    prediction()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xTVaZOIDjF6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = CustomDataset('validation')\n",
        "val_dataset[1]"
      ],
      "metadata": {
        "id": "T3ZTKk6BmKBs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}